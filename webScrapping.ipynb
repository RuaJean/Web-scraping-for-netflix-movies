import requests
from bs4 import BeautifulSoup
import time
import random
import csv

# Definimos un header con el User-Agent de una Mac para evitar baneos
user_agents = [
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'
]

# Función para obtener headers aleatorios
def obtener_headers():
    headers = {
        'User-Agent': random.choice(user_agents)
    }
    return headers

# Función para obtener los títulos y los enlaces de las películas en cada página
def obtener_enlaces_peliculas(url, intentos=0):
    max_reintentos = 5
    try:
        response = requests.get(url, headers=obtener_headers())
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            movie_links = []
            movie_titles = soup.find_all('div', class_='movie-title')

            if not movie_titles:
                print(f"[DEBUG] No se encontraron películas en la URL: {url} - Reintento {intentos+1}/{max_reintentos}")
                if intentos < max_reintentos:
                    time.sleep(2)
                    return obtener_enlaces_peliculas(url, intentos + 1)
                else:
                    print(f"[ERROR] No se encontraron películas después de {max_reintentos} reintentos.")
                    return []

            for title in movie_titles:
                a_tag = title.find('a')
                if a_tag:
                    enlace = a_tag.get('href')
                    nombre = a_tag.get('title').strip()
                    if not enlace.startswith('https'):
                        enlace = f"https://www.filmaffinity.com{enlace}"
                    movie_links.append({'name': nombre, 'link': enlace})
            return movie_links
        else:
            print(f"[ERROR] Status Code: {response.status_code}")
            return []
    except Exception as e:
        print(f"[ERROR] Ocurrió un error al obtener los enlaces de películas: {str(e)}")
        return []

# Función para obtener la información de la película con reintentos y depuración adicional
def obtener_criticas(url_pelicula, intentos=0, max_reintentos=3):
    try:
        print(f"[INFO] Accediendo al enlace de la película: {url_pelicula} (Intento {intentos+1}/{max_reintentos})")
        response = requests.get(url_pelicula, headers=obtener_headers())
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # Buscar el dl con la clase 'movie-info'
            movie_info = soup.find('dl', class_='movie-info')

            if movie_info:
                print(f"[DEBUG] Verificando el contenido de movie-info en {url_pelicula}...")

                try:
                    original_title = movie_info.find('dt', text='Título original').find_next_sibling('dd').get_text(strip=True)
                except AttributeError:
                    print(f"[ERROR] No se pudo extraer el título original.")
                    original_title = None

                try:
                    year = movie_info.find('dt', text='Año').find_next_sibling('dd').get_text(strip=True)
                except AttributeError:
                    print(f"[ERROR] No se pudo extraer el año.")
                    year = None

                try:
                    duration = movie_info.find('dt', text='Duración').find_next_sibling('dd').get_text(strip=True)
                except AttributeError:
                    print(f"[ERROR] No se pudo extraer la duración.")
                    duration = None

                try:
                    country = movie_info.find('dt', text='País').find_next_sibling('dd').get_text(strip=True)
                except AttributeError:
                    print(f"[ERROR] No se pudo extraer el país.")
                    country = None

                try:
                    genre = movie_info.find('dt', text='Género').find_next_sibling('dd').get_text(strip=True)
                except AttributeError:
                    print(f"[ERROR] No se pudo extraer el género.")
                    genre = None

                # Comprobando si todos los datos requeridos están presentes
                if not original_title or not year or not duration or not country or not genre:
                    if intentos < max_reintentos:
                        print(f"[WARNING] No se encontraron todos los datos necesarios. Reintentando... ({intentos+1}/{max_reintentos})")
                        time.sleep(2)
                        return obtener_criticas(url_pelicula, intentos + 1, max_reintentos)
                    else:
                        print(f"[ERROR] No se encontraron todos los datos después de {max_reintentos} reintentos.")
                        return None

                # Obtener el enlace de las críticas (si existe)
                review_container = soup.find('div', id='review-container')
                if review_container:
                    reviews_box = review_container.find('div', id='movie-reviews-box')
                    if reviews_box:
                        a_tag = reviews_box.find('a', text=lambda t: 'críticas' in t.lower())
                        if a_tag:
                            href_reviews = a_tag.get('href')
                            review_count = reviews_box.get_text(strip=True).split()[0]

                            # Construir el enlace completo si es relativo
                            if not href_reviews.startswith('https'):
                                href_reviews = f"https://www.filmaffinity.com{href_reviews}"

                            return {
                                'original_title': original_title,
                                'year': year,
                                'duration': duration,
                                'country': country,
                                'genre': genre,
                                'reviews_link': href_reviews,
                                'reviews_count': review_count
                            }

                # Si no hay críticas, devolver información básica
                return {
                    'original_title': original_title,
                    'year': year,
                    'duration': duration,
                    'country': country,
                    'genre': genre,
                    'reviews_link': None,
                    'reviews_count': '0'
                }
            else:
                print(f"[WARNING] No se encontró 'movie-info' en la página de la película: {url_pelicula}")
                return None
        else:
            print(f"[ERROR] No se pudo obtener la página de la película, código de estado: {response.status_code}")
            return None
    except Exception as e:
        print(f"[ERROR] Ocurrió un error al obtener la información de la película: {str(e)}")
        return None

# Función para obtener detalles de las críticas desde el enlace de críticas
def obtener_detalles_criticas(url_criticas, intentos_max=3):
    try:
        criticas = []
        base_url, review_id = url_criticas.rsplit('/', 1)
        base_url = base_url.rsplit('/', 1)[0]
        for intento in range(1, intentos_max + 1):
            new_url_criticas = f"{base_url}/{intento}/{review_id}"
            print(f"[INFO] Accediendo al enlace de críticas página {intento}: {new_url_criticas}")

            response = requests.get(new_url_criticas, headers=obtener_headers())
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')

                reviews_wrapper = soup.find('div', class_='reviews-wrapper')
                if reviews_wrapper:
                    reviews = reviews_wrapper.find_all('div', class_='fa-shadow movie-review-wrapper rw-item')
                    print(f"[INFO] {len(reviews)} críticas encontradas en la página {intento}.")
                    for review in reviews:
                        user = review.find('div', class_='mr-user-nick').get_text(strip=True)
                        score = review.find('div', class_='user-reviews-movie-rating').get_text(strip=True)
                        review_text = review.find('div', class_='review-text1').get_text(strip=True)

                        criticas.append({
                            'user': user,
                            'score': score,
                            'text': review_text
                        })

                        print(f"[DEBUG] Usuario: {user}, Puntuación: {score}")
                        print(f"[DEBUG] Texto de la crítica: {review_text[:100]}...")
            else:
                print(f"[ERROR] No se pudo acceder al enlace de críticas, código de estado: {response.status_code}")
                break

        return criticas
    except Exception as e:
        print(f"[ERROR] Ocurrió un error al obtener los detalles de las críticas: {str(e)}")
        return []

# URL base para las películas de Netflix
url_base = "https://www.filmaffinity.com/es/category.php?id=new_netflix&page="

# Inicializamos variables
pagina = 1
peliculas_totales = []

# Crear archivo CSV y escribir encabezados en inglés
csv_filename = 'movies_and_reviews.csv'
with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Movie', 'Movie Link', 'Year', 'Duration', 'Country', 'Genre', 'Reviews Link', 'Number of Reviews', 'User', 'Score', 'Review Text'])

# Bucle para iterar sobre la paginación de las películas
while True:
    try:
        url = url_base + str(pagina)
        print(f"[INFO] Scrapeando la página {pagina}...")
        print(f"[DEBUG] Enlace de la página: {url}")

        enlaces_peliculas = obtener_enlaces_peliculas(url)

        if not enlaces_peliculas:
            print("[INFO] No hay más películas.")
            break

        for pelicula in enlaces_peliculas:
            print(f"[INFO] Procesando película: {pelicula['name']} - Enlace: {pelicula['link']}")

            criticas_info = obtener_criticas(pelicula['link'])
            if criticas_info:
                pelicula.update(criticas_info)

                # Obtener detalles de las críticas si existen
                detalles_criticas = []
                if criticas_info['reviews_link']:
                    detalles_criticas = obtener_detalles_criticas(criticas_info['reviews_link'])

                # Escribir las críticas o, si no hay críticas, la película sin ellas
                with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:
                    writer = csv.writer(file)
                    if detalles_criticas:
                        for critica in detalles_criticas:
                            writer.writerow([
                                pelicula['name'],
                                pelicula['link'],
                                pelicula['year'],
                                pelicula['duration'],
                                pelicula['country'],
                                pelicula['genre'],
                                pelicula['reviews_link'],
                                pelicula['reviews_count'].replace(" críticas", ""),
                                critica['user'],
                                critica['score'],
                                critica['text']
                            ])
                    else:
                        writer.writerow([
                            pelicula['name'],
                            pelicula['link'],
                            pelicula['year'],
                            pelicula['duration'],
                            pelicula['country'],
                            pelicula['genre'],
                            pelicula['reviews_link'],
                            pelicula['reviews_count'],
                            None, None, None  # Sin críticas
                        ])

        pagina += 1
        time.sleep(2)

    except Exception as e:
        print(f"[ERROR] Ocurrió un error en el bucle principal: {str(e)}")
        break

print(f"[INFO] Proceso completado. Datos guardados en {csv_filename}")
